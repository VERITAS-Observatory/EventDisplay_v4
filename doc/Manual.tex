%*-- Author :    G.Maier   2011/09/25
\documentclass[titlepage,a4paper,twoside,11pt]{report}
% floating objects
\usepackage{float}
% floating object placements
\usepackage{afterpage}
% use more of a page
\usepackage{a4wide}
% sort citation numbers
\usepackage{cite}
% more symbols, e.g. \blacksquare, \blacktriangle
\usepackage{amssymb}
% \usepackage{lscape}  % landscape with \begin{landscape} \end{landscape}
% header with lines etc.
\usepackage{fancyheadings}
\usepackage{longtable} % table longer than one page
\usepackage{tabularx}
\usepackage{booktabs} % toprule, etc.

% captions in italic
\usepackage{caption2}
\renewcommand*\captionfont{\slshape}
\renewcommand*\captionlabelfont{\upshape}
% versions
\usepackage{version}
 \includeversion{TEV}
 \excludeversion{GREEN}
 \excludeversion{LMX}
 \excludeversion{HMX}
 \excludeversion{EGRET}
\excludeversion{TAYLOR}

\usepackage[pdftex]{graphicx}
\pdfcompresslevel=0   % pdf-file should (not) be compressed
\usepackage[pdftex]{color}
% make bookmarks, don't show bookmarks when opening the file
\usepackage[pdftex,colorlinks=false,bookmarks=false,bookmarksopen=false]{hyperref}
\pdfinfo
  {
      /Title (promo.pdf)
      /Creator (TeX)
      /Producer (pdfTeX)
      /Author (Gernot Maier)
      /CreationDate (D:20110925180100)
      /ModDate (D:20110925180100)
      /Subject (manual)
      /Keywords (gamma rays, analysis, VERITAS, CTA)
  }
%  \usepackage{thumbpdf}

%%%%%%%%%%%%%%%
%% pdflatex configure file ????
%% To include a graphics file \includegraphics{filename_to_include}
%% convert eps files with epstopdf

% no header and no page number, ersetzt \cleardoublepage
\newcommand{\clearemptydoublepage}{\newpage{\pagestyle{empty}\cleardoublepage}}

% new headers
\pagestyle{fancyplain}
\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection\ #1}}
\lhead[\fancyplain{}{\bfseries\thepage}]{\fancyplain{}{\bfseries\rightmark}}
\rhead[\fancyplain{}{\bfseries\leftmark}]{\fancyplain{}{\bfseries\thepage}}
\cfoot{\fancyplain{\thepage}}

% \marginlabel, replaces \marginpar 
\newcommand{\marginlabel}[1]{\mbox{}\marginpar{\raggedleft\hspace{0pt}{\color{cyan}#1}}}
% degrees
\newcommand{\Grad}{$^{\circ}$}
%% new color
\definecolor{Grau}{gray}{0.905}
\usepackage{listings}
\lstset{language=C++, basicstyle=\footnotesize,backgroundcolor=\color{Grau},aboveskip=\bigskipamount,belowskip=\bigskipamount,frame=TB}
\renewcommand{\textfraction}{0.02}
\renewcommand{\floatpagefraction}{0.70}

% spelling
\hyphenation{CRES KRETA KASCADE GHEISHA Karls-ruhe CORSIKA}

% depth of sub sections
\setcounter{secnumdepth}{3}

%indent
\setlength{\parindent}{0pt}

\begin{document}

% ########### title page ###########################

\include{Manual_Title}

% ############ Vorspann  ###########################

\pagenumbering{roman}
\thispagestyle{empty}


\setcounter{page}{1}
\tableofcontents
\clearemptydoublepage


% ############ main part #####################
\pagenumbering{arabic}

\chapter{Documentation}

EVNDISP is work-in-progress and the documentation is not in the state it is supposed to be. 
Apart from the information in this manual, other sources for help are:

\subsection*{README files}

\begin{description}
\item[INSTALL:]   information on installation the analysis package, dependencies, environmental variables
\item[README.CTA :]  short description of a typical CTA analysis
\item[README.VTS:]      description of a typical VERITAS analysis
\item[AUTHORS:]		author description
\end{description}

\noindent Description and command line options for the different software parts:

\begin{description}
\item[README.EVNDISP:]
\item[README.MSCW\_ENERGY:]
\item[README.ANASUM:]
\item[README.EFFECTIVEAREA:]
\item[README.ANALYSISLIBRARY:]
\item[README.SCRIPTS:]
\item[README.MACROS:]
\end{description}

\subsection*{WIKI pages}

The EVNDISP manual for VERITAS users: 

\url{http://veritas.sao.arizona.edu/wiki/index.php/Eventdisplay\_Manual}

\clearemptydoublepage

\chapter{Introduction}

\noindent eventdisplay is a complete package for VERITAS and CTA analysis (whatever 'complete' means...)
The package consists of several analysis steps and tools:

\begin{enumerate}
\item evndisp (calibrate and parametrize images, event reconstruction, stereo analysis)
\item mscw\_energy (use lookup tables to produce msw, msl, and energies)
\item anasum (produce maps and calculate analysis results)
\item shared library tools and macros  (produce the energy spectrum and integral fluxes, plot maps, plot sensitivities, etc.) 
\item makeEffectiveArea (calculate effective areas)
\item trainTMVAforGammaHadronSeparation (tools to train MVA and optimize cuts)
\item ...
\end{enumerate}

This is a very incomplete manual, started in September 2011. Please help updating it.

The original developers are Gernot Maier (DESY) and Jamie Holder (University of Delaware). 
A large number of people contributed, among others: ..

% #############################################
%
% ############ Installation and auxiliary data files ######
%
% #############################################

\chapter{Installation and auxiliary data files}
\label{SECTION.ENVIRON}

The installation, compilation, and needed or useful environmental variables are described in detail in the file {\it README/INSTALL} in the EVNDISP source code.

\section{Auxiliary data files}
\label{SECTION.AUXFILE}

The auxiliary data files contain information needed for the analysis. 
This might be files describing the detector geometry, lookup tables, effective areas file, etc.
We assume in the following that these files are located in the
directory {\it \$OBS\_EVNDISP\_ANA\_DIR} where {\it OBS} is your
observatory (i.e. {\it VERITAS} or {\it CTA}).

\subsection{Detector description}

\subsubsection{VERITAS}

The detector description is read from a configuration file in GrIsu style. There are examples for the different detector configurations (e.g. pre or post-upgrade) in 
the tar ball with all the analysis files. 
Check the files in  {\it \$OBS\_EVNDISP\_ANA\_DIR/*.cfg}.

\subsubsection{CTA}

No detector description is needed, since the telescope and array description is read directly from the DST file ({\it telconfig} tree).
The converter from hessio to EVNDISP DST format needs a subarray file, a simple list of telescopes to be selected from the corresponding hyper array. 
The subarray files for prod1 can be found in {\it \$OBS\_EVNDISP\_ANA\_DIR/DetectorGeometry/*.lis}.

\subsection{Analysis parameters files}

Detailed description of these files can be found in the corresponding chapters of the different analysis stages.

% #############################################
%
% ############ Trace integration and image analysis  ###
%
% #############################################

\chapter{eventdisplay - calibration, image analysis and stereo reconstruction}

\section{Trace integration}

\section{Calibration}

\subsection{Low gain calibration}

{\bf\color{red}VERITAS only}

For the calibration of the low-gain chain two values are important: the low-gain pedestal and the ratio between low and high-gain channel (usually between 5 and 6).

\subsubsection{Low-gain pedestal calibration}

Pedestal values of the low-gain chain are not sensitive to changes in the NSB as the high-gain chain.
Therefore new calibration is only necessary after changes in the electronics, e.g. the after swapping a FADC board.
For a list of good low-gain calibration runs and the valid time ranges look the VERITAS wiki page {\it Low gain calibration}.
There is a list of documents linked to this page, valuable for background information.

Low-gain calibration runs are taken by raising the HV in part of the cameras - therefore a pair of flasher runs is needed for
the calibration (exception is the first run, 36862).
The calculation of the low-gain pedestal values consists of two steps: 
i) the calculation of the pedestal values and ii) merging these files into single pedestal files.

Step 1 (script is in \$EVNDISPSYS/scripts/VTS)
\begin{lstlisting}
VTS.EVNDISP.analyse_pedestal_events <telescope number> \\
				<sourcefile> [runnumber] [lowgain]

use parameter lowgain=1 to calculate low-gain pedestals

    VTS.EVNDISP.analyse_pedestal_events -1 /d20110213/55083.cvbf 55083 1

\end{lstlisting}

Results are written into the calibration directory in \$VERITAS\_EVNDISP\_ANA\_DIR, 
files with suffixes {\it.lped} and {\it .lped.root}.

Step 2: merge files into a single low-gain pedestal file

Use the EVNDISP shared library and the class {\it VPedestalCombineLowGainFiles}:

\begin{lstlisting}
root [0] .L $EVNDISPSYS/lib/libVAnaSum.so
root [1] VPedestalCombineLowGainFiles a;
root [2] a.combineLowGainPedestalFileForAllTelescopes( unsigned int iNTel, \\
string iCalibrationDirectory, \\
string iRun1, string iRun2, string iOutRun );
\end{lstlisting}

The list of run numbers and the corresponding valid ranges are listen in the file
{\it calibrationlist.LowGain.dat} in the calibration direction in \$VERITAS\_EVNDISP\_ANA\_DIR:

\begin{lstlisting}

LOWGAINPED <TELESCOPE> <RUNSTART> <RUNSTOP> <LOW GAIN PEDESTAL RUN NUMBER>
(all comparisons are >= and <=)

RUNS 55975 and 55978 (128 samples)
* LOWGAINPED -1 46642 60000 5597578 
....
....

\end{lstlisting}

\subsubsection{Low-gain multiplicator}

\section{Image analysis}

\subsection{Log-likelihood fitting of images}

The status of the covariance matrix is returned and saved to the image parameter tree {\it tpars} in the variable {\it Fitstat}. 
No fitting has been applied to images with $Fitstat=-1$ 

\section{Image cleaning}

\chapter{Displaying events}

eventdisplay can be used to display events. 
Camera images of integrated charges, timing and calibration values, image cleaning and parameterization, core and direction reconstruction results can be displayed. 
There are example scripts for display files, see

\begin{itemize}
\item for CTA:  \$EVNDISP/scripts/CTA/CTA.EVNDISP.display
\item for VERITAS:  \$EVNDISP/scripts/VTS.EVNDISP.display
\end{itemize} 

Note that the display can be a bit slow for arrays with a large number of telescopes.

\chapter{mscw\_energy - using lookup tables}

\section{Energy reconstruction}
\label{SECTION.ENERGYRECONSTRUCTION} 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%   GAMMA HADRON SEPARATION 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	

\chapter{Gamma/hadron separation}

\section{Cut parameters}
\label{SECTION.CUTS.PARAMETERFILE}

\begin{longtable}{r  r  r r  p{4cm}}
\toprule
\hfill
Option & Number of & Allowed & Default & Description  \\
             & parameters & value(s) & value(s) & \\
\midrule
cutselection & 2  & &  & type of gamma/hadron and direction cut \\
\multicolumn{2}{r@{}}{Parameter \#1} & ?     & 0 & gamma/hadron cut id \\
\multicolumn{2}{r@{}}{Parameter \#2} & 0-5 & 0  & direction cut id: fixed $\Theta^2$ cut (0, needs parameter {\it theta2cut}), energy dependent $\Theta^2$ from a function read from a IRF file (1, needs parameter {\it theta2file}), from a IRF graph (2, needs parameter {\it theta2file} and option {\it IRF}), all other values: experimental (3-5, TMVA) \\
angres & 1 & $]0,100]$ & 0 & containment probability for energy dependent $\Theta^2$ cut (in \%) \\
\bottomrule
\caption{Parameter definition and range for gamma/hadron cut files. This is used for example in the effective area calculation or for data analysis.}
\end{longtable}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}
\caption{Gamma/hadron cut selector values. They consist of two digits: ID1+ID2*10}
\begin{tabularx}{\linewidth}{c X }
\toprule
\hfill
gamma/hadron cut  & Description \\
selector & \\
\midrule
\multicolumn{2}{@{}l@{}}{ID2} \\
     0 & apply gamma/hadron cuts on parameters in given data tree \\
     1 & apply gamma/hadron cuts on probabilities given by a friend to the data tree (e.g. random forest analysis) \\
     2 & same as 2 \\
     3 & apply cuts on probabilities given by a friend to the data tree already at the level of the event quality level (e.g. of use for analysis of certain binary phases only) \\
     4 & TMVA gamma/hadron separation \\
     
\midrule
\multicolumn{2}{@{}l@{}}{ID1} \\

    0 & apply cuts on MSCW/MSCL (mean reduced scaled width/length) \\
    1 & apply cuts on mean width/length (no lookup tables necessary) \\
     2 & no cut applied (always passed) \\
     3 & apply cuts on MWR/MLR (mean scaled width/length)  \\
     
 \midrule    
 \multicolumn{2}{@{}l@{}}{Example:} \\

     0 & apply MSCW/MSCL cuts (default) \\
     22 & apply event probability cuts \\
    10  & apply cuts from a tree AND apply MSCW/MSCL cuts \\
    40 & use TMVA  AND apply MSCW/MSCL cuts    \\
\bottomrule
\end{tabularx}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{makeEffectiveArea - instrument response functions}

Instrument response functions (IRF), i.e. effective areas and angular, core and energy resolution and bias curve can be calculated using {\it makeEffectiveArea}. 

\begin{small}
\begin{longtable}{r  r  r r  p{3.5cm}}
\toprule
\hfill
Option & Number of & Allowed & Default & Description  \\
             & parameters & value(s) & value(s) & \\
\midrule
%\multicolumn{6}{c}{keine Atmosph"aren-Korrektur} \\
\midrule
FILLINGMODE  & 1 & 0,1,2,3 & 0 & filling of IRFs: fill all IRFs (0), resolution plots only (1), angular resolution plot only (2), effective areas only (3) \\

ENERGYRECONSTRUCTIONMETHOD & 1 & 0,1 & 0 &  energy reconstruction method (see \ref{SECTION.ENERGYRECONSTRUCTION}) \\

ENERGYAXISBINS & 1 & $>0$ & 60 & number of bins on $\log_{10}$ energy axis \\

ENERGYRECONSTRUCTIONQUALITY & 1 & 0,1 & 0 &  \\

AZIMUTHBINS & 1 & 0,1 & 1 & define azimuth bins and calculate IRFs in each azimuth bin. Bins are hardwired with a bin width of $22.5^{\mathrm{o}}$ (16 bins), bin 17 contains the full azimuth range \\

ISOTROPICARRIVALDIRECTIONS & 1 & 0,1 & 0 & input MC are simulated with random direction (wobble) offsets (use for gamma rays only) \\

TELESCOPETYPECUTS & 1 & 0,1 & 0 & apply telescope type dependent cuts {\color{red} CHECK! STILL USEFUL?}\\

FILLMONTECARLOHISTOS & 1 & 0,1 & 0 & fill histograms with MC spectra only (no IRF calculation) \\

ENERGYSPECTRUMINDEX  & 3 &  &  & reweight events to this set of spectral indexes \\
& \#1 & $>0$ & 1 & number of different spectral indexes \\
& \#2 & $>0.$ & 2.0 & lower value \\ 
& \#3 & $>0.$ & 0.1 & step size \\

CUTFILE & 1 & & & cut file (full path, see \ref{SECTION.CUTS.PARAMETERFILE}) \\

SIMULATIONFILE\_DATA & 1  & & & simulation data file (mscw file) \\

SIMULATIONFILE\_MCHISTO  & 1  & & & data file with thrown events. This can be either a full mscw file (slow) or a file with the filled MC histograms \\

\bottomrule
\caption{Parameters in the run parameter file for the IRF calculation.}
\end{longtable}
\end{small}


% #############################################
%
% ############ CTA analysis ######################
%
% #############################################

\chapter{CTA analysis}

\section{General concept}

We assume in the following that CTA simulations are produced and available in the {\it hessio} format. 
Eventdisplay does not depend on {\it hessio} and the reading of any future CTA data format can easily be implemented.

To use Eventdisplay for CTA analyses, the {\it simtel.gz} files have first to be converted into ROOT format (called in the following DST format) using the converter tool described below. 
The analysis then proceed in several steps.
Note that the analysis must be done for each subarray separately, even if there are more telescopes in the simtel files (concept of hyper arrays).
The following list shows the exectuables and macros needed to produce CTA sensitivity files:

\begin{description}
\item[CTA.convert\_hessio\_to\_VDST]
        convert simtel.gz files into ROOT format

\item[eventdisplay]
	FADC trace integration (if needed); image cleaning; calculation of image parameters; reconstruction of the direction and core; display

\item[mscw\_energy]
        train and use lookup tables to estimate energies and mean scaled parameters (note that these are two separate steps)
        
\item[trainTMVAforGammaHadronSeparation]
        train MVA for gamma/hadron suppression (e.g. using boosted decision trees or box cuts)

\item[makeEffectiveArea]
        calculate effective areas and instrument response functions (e.g. angular and energy resolution; migration matrix, etc)

\item[sensitivity.C]
        ROOT macro to calculate signal and background events after pre-cuts and to plot sensitivity curves

\item[writeCTAWPPhysSensitivityFiles]
        write sensitivity files (CTA WP-PHYS style root file)
        
\end{description}

There are shell scripts to simplify these steps in the {\it \$EVNDISP/scripts/CTA} directory (see description in section \ref{SECTION.CTASCRIPTANALYSIS}.
They allow analyze a large amount of simulations files on a computing cluster\footnote{Most of these scripts work fine on the DESY batch system, they might need some adjustment for other computing environments}.
It is easy with their help to analyze many simtel.gz files for several subarrays, offsets, cuts, and so on. 
The scripts expect several environmental variables to be set during execution time, see section \ref{SECTION.ENVIRON} and the readme file {\it README/INSTALL} for details.

\subsection{Array configuration - subarray file}

Many of the CTA simulation files contain data for a so called hyper array 
(a large number of telescopes have been simulated, a sub-set of the telescope data is analyzed).
To select the actual array, the corresponding telescopes have to be specified in a ASCII text file (called {\it subarray} file in the following).
The  example file below selects an array consisting of telescopes 63, 19, 67, and 33, each telescope
with a field of view of 8 degrees (the FOV is reduced with respect to the simulated FOV by setting the corresponding pixels dead; for no FOV cut, set this value to a very large number):

\begin{lstlisting}
63 8
19 8
67 8
33 8
\end{lstlisting}

Subarray files for most of the arrays used in the CTA sensitivities studies are part of the analysis file package 
(see \ref{SECTION.AUXFILE}). The subarray files for prod1 can be found in 

{\it \$OBS\_EVNDISP\_ANA\_DIR/DetectorGeometry/*.lis}.

Note that a subarray file is always needed, even if all telescopes from the hessio file are read out.

%###########################################################################
\subsection{Step 1: Converter}

In this step a simulation data file in {\it hessio} format ({\it simtel.gz}) is converted to the EVNDISP DST format 
(see \ref{SECTION.DST}). These are the possible options to run the converter:

\begin{small}
\begin{lstlisting}
$EVNDISPSYS/bin/CTA.convert_hessio_to_VDST

CTA.convert_hessio_to_VDST: A program to convert hessio data to EVNDISP DST files (v.4.00)
=====================================================================

Syntax: ./bin/CTA.convert_hessio_to_VDST [ options ] [ - | input_fname ... ]
Options:
   -v               (More verbose output)
   -q               (Much more quiet output)
   -s               (Show data explained)
   -S               (Show data explained, including raw data)
   --history (-h)   (Show contents of history data block)
   -i               (Ignore unknown data block types)
   --max-events n   (Skip remaining data after so many triggered events.)
   -a subarray file (list of telescopes to read with FOV.)
   -o dst filename  (name of dst output file)
   -f on=1/off=0    (write FADC samples to DST file;default=0)
   -r on=1/off=0    (apply camera plate scaling for DC telescopes; default=1)
   -d <nbits dyn.>  (dynamic range of readout (e.g. 12 for 12 bit. Switch to low gain)
\end{lstlisting}
\end{small}


The following options are mandatory: -a (subarray  file, see next paragraph) and -o (DST output file).
Note the limitations of the DST format (\ref{SUBSECTION.DST.LIMITATIONS}).

For a typical run, the following command line should be used:

\begin{lstlisting}
./CTA.convert_hessio_to_VDST -a subArray.list -o dstfile.dst.root \ 
          gamma_run12241.simhess.gz  
\end{lstlisting}

For FADC analysis, add the option {\it -f 1}. Note again the limitations of the DST format (\ref{SUBSECTION.DST.LIMITATIONS}).

%###########################################################################
\subsection{Step 2: Display (event-by-event)}

It is always useful to look at events in the display. 
To do this, it is best to select a small subarray with the {\it -teltoana} option in evndisp.
The display might otherwise be quite slow in responding to your input due to the large number of objects to be drawn.

A typical command line to look at events might be:

\begin{lstlisting}
$EVNDISPSYS/bin/evndisp -display=1 \
-reconstructionparameter ./EVNDISP.reconstruction.runparameter \
-l2setspecialchannels nofile \
-sourcefile tt.v2.root 
\end{lstlisting}

%###########################################################################
\subsection{Step 3, Eventdisplay: FADC integration, calibration, image analysis and stereo reconstruction}

{\bf section missing}

%###########################################################################

\subsection{Plotting effective areas in instrument response functions}
\label{SEC.PLOTEFFAREA}

Plotting the effective areas and instrument response functions using
the shared library from evndisplay: 

\begin{lstlisting}
% root
root [0] .L $EVNDISPSYS/lib/libVAnaSum.so
root [1] VPlotInstrumentResponseFunction a;
root [2] a.addInstrumentResponseData("gamma_onSource.E_ID0.eff-0.root")
root [3] a.plotEffectiveArea()
root [4] a.plotAngularResolution()
root [5] a.plot...
\end{lstlisting}

%###########################################################################
\section{Automized CTA analysis using scripts}
\label{SECTION.CTASCRIPTANALYSIS}

All bash scripts described in the following can be found in {\it \$EVNDISPSYS/scripts/CTA}.

\subsection{Directory structure of analysis products and log files}

The following scripts expect a certain directory structure for data products and log files.
Following this structure simplifies the analysis significantly.

\begin{itemize}
\item different simulations sets are distinguished by their name. This name has to be chosen at the beginning of the analysis, and then used consistently through all steps (e.g. {\it cta-ultra3} or {\it ISDC3700m}). In the following, the placeholder {\bf SIMDATASET} is used in all directory names and examples of command lines.
\item  analysis results (in ROOT format)  are written to \\
{\it \$CTA\_DATA\_DIR/analysis/SIMDATASET}
\item analysis log files to {\it \$CTA\_LOG\_DIR/analysis/SIMDATASET}
\item results for individual subarrays written in sub directories \\
 (e.g. {\it \$CTA\_DATA\_DIR/analysis/SIMDATASET/E/})
 \item the configuration files for the different subarrays are expected to be in \\
 {\it \$OBS\_EVNDISP\_ANA\_DIR/DetectorGeometry/}.
\end{itemize}

\subsubsection{Runparameter files for scripts}
\label{SEC:scriptsRunParameter}

The information on directory names, reconstruction methods etc are used in several scripts. 
To simplify this, some parameters are read from a run parameter file. 
An example can be found in {\it\$OBS\_EVNDISP\_ANA\_DIR/ParameterFiles/scriptsInput.runparameter}.
Note that the values are read out by a simple {\it grep} command, so no repetitions are allowed. 

{\bf This is work in progress}

\begin{description}
\item[MSCWSUBDIRECTORY] [string]: name of sub directory with products from the lookup table analysis
\item[TMVASUBDIR] [string]: name of sub directory where results from TMVA training are written 
\item[EFFAREASUBDIR] [string]: name of sub directory for results from effective area calculation
\item[RECID] [integer]: quality cuts selector used in lookup table analysis (default is 0)
\item[ENERGYRECONSTRUCTIONMETHOD] [integer]: ID of energy reconstructed method
\item[NIMAGESMIN] [integer]: cut on minimum number of images applied in analysis (default is 2)
\end{description} 

Example:
\begin{lstlisting}
MSCWSUBDIRECTORY Analysis-NM2-ID0-d20120507
TMVASUBDIR BDT-NM2-ID0-ErecS-d20120510
EFFAREASUBDIR EffectiveArea-NM2-ID0-ErecS-d20120510

RECID 0
ENERGYRECONSTRUCTIONMETHOD 1
NIMAGESMIN 2
\end{lstlisting}

\subsection{Off-axis analysis}

The off-axis analysis (creating lookup tables and training the gamma-hadron separator at different camera offsets) is called {\it cone10}. 
The offset bins are hardwired in the bash scripts to the following values  (mean and intervals):

\begin{lstlisting}
 OFFMIN=( 0.0 1.0 2.0 3.00 3.50 4.00 4.50 5.00 5.50 )
 OFFMAX=( 1.0 2.0 3.0 3.50 4.00 4.50 5.00 5.50 6.00 )
 OFFMEA=( 0.5 1.5 2.5 3.25 3.75 4.25 4.75 5.25 5.75 )    (mean value)
\end{lstlisting}

%###########################################################################
\subsection{Step 1 \& 3 combined: convert simtel file \& run eventdisplay}

Run the converter (simtel to DST) and eventdisplay analysis for a list of subarrays.

\begin{lstlisting}
./CTA.EVNDISP.sub_convert_and_analyse_MC_VDST.sh

 <sub array list>          text file with list of subarray IDs
  
 <list of simtelarray files>  

 <particle>                gamma_onSource , gamma_cone10, proton , ...

 <data set>                e.g. cta-ultra3, ISDC3700m, ...
 
<0/1>  [1 = keep DST.root files (default off=0=delete DST.root files after analysis)]
\end{lstlisting}

NOTE: The image cleaning thresholds can be specified in the file 

{\it \$OBS\_EVNDISP\_ANA\_DIR/ParameterFiles/EVNDISP.reconstruction.runparameter}

file either for all telescopes to the same values or for each telescope type seperately
(see section XXXX). 

\paragraph{Input}
Simulation file from simtel. \\
Parameter files for image and array analysis:\\
{\it \$OBS\_EVNDISP\_ANA\_DIR/ParameterFiles/EVNDISP.reconstruction.runparameter}

\paragraph{Output}

One ROOT and one log file for each simtel file with results from image and array analysis in e.g. \\
{\it \$CTA\_DATA\_DIR/analysis/SIMDATASET/gamma\_onSource/} and \\
{\it \$CTA\_LOG\_DIR/analysis/SIMDATASET/gamma\_onSource/} 

%###########################################################################
\subsection{Step 4: mscw\_energy}

If you use standard configurations maybe some lookup tables already exist (ask Gernot or Heike where you could find them).
If not, you have to create them from gamma-ray simulations yourself.

\subsubsection{Creating lookup tables}

Lookup tables are used for mean scaled with, length and energy reconstruction.

\begin{lstlisting}
CTA.MSCW_ENERGY.sub_make_tables.sh <table file name> <recid> <subarray list> \
<onSource/cone10> <data set>

  <table file name>  name of the table file (to be written; without .root)
  <recid>            reconstruction ID according to EVNDISP.reconstruction.parameter
  <subarray list>    text file with list of subarray IDs
  <onSource/cone10>    calculate tables for on source or different wobble offsets
  <data set>         e.g. cta-ultra3, ISDC3700m, ... 
\end{lstlisting}

\paragraph{Input}
eventdisplay ROOT files from previous analysis step.

\paragraph{Output}

One lookup table files (ROOT format) to be found in \\
{\it \$CTA\_DATA\_DIR/analysis/SIMDATASET/Tables/}. \\
One log file in \\
{\it \$CTA\_LOG\_DIR/analysis/SIMDATASET/Tables/}.

\subsubsection{Off-axis lookup tables}

The off-axis tables produced in different wobble offset bins have to be combined into a single file before any further steps:

\begin{lstlisting}
CTA.MSCW_ENERGY.combine_tables.sh <combined table file name> <subarray list> \
<input table file name> <output directory> <data set> 

  <combined table file name>  name of the table combined file (without .root)

  <subarray list>             text file with list of subarray IDs

  <input table file name>     name of the input table name (beginning of...)

  <output directory>          directory for combined tables
\end{lstlisting}

Copy or move the created tables to your directory 
\$OBS\_EVNDISP\_ANA\_DIR/Tables/

\subsubsection{Using lookup tables}

Run mscw\_energy for estimating the energy of each
event: 

\begin{lstlisting}
CTA.MSCW_ENERGY.sub_analyse_MC.sh <tablefile> <recid> <subarray list> <particle> \
<data set> [wildcard]

  <tablefile>     table file name (without .root)
                  expected file name: xxxxxx-SUBARRAY.root; \
                    SUBARRAY is added by this script
  <recid>         reconstruction ID
  <subarraylist > text file with list of subarray IDs
  <particle>      gamma_onSource / gamma_cone10 / electron / proton / helium
  <data set>      e.g. ultra, ISDC3700m, ...

optional (for a huge amount of MC files):
  [wildcard]     used in the < CTA.MSCW_ENERGY.subParallel_analyse_MC.sh > script
\end{lstlisting}

To submit jobs for different particle types, use \\
{\it \$EVNDISPSYS/scripts/CTA/CTA.subAllParticles\_analyse\_MC.sh}.

\paragraph{Input}
Lookup tables \\
eventdisplay ROOT files from previous analysis step.

\paragraph{Output}

One or several ROOT files for each particle type in \\
{\it \$CTA\_DATA\_DIR/analysis/SIMDATASET/Analysis/}. \\
Log files will be in \\
{\it \$CTA\_LOG\_DIR/analysis/SIMDATASET/Analysis/}. \\
You can combine many ROOT files from the first analysis step (converter+eventdisplay) into a single large ROOT file.

%###########################################################################
\subsection{Step 5: Optimize cuts or train MVA}

Gamma-hadron separation is based on TMVA and needs to be trained for
each subarray due to their different layouts. Here an example how to
do this for Boosted Decicion Trees (BDT).
Input to the training are the files produced during the lookup table analysis 
(containg shower direction and core, MSCW, MSCL, reconstructed energy, etc.).

\begin{lstlisting}
CTA.TMVA.sub_train.sh <subarray list> <onSource/cone10> <data set> <analysis parameter file>

  <subarray list>   text file with list of subarray IDs

  <onSource/cone10>    calculate tables for on source or different wobble offsets

  <data set>         e.g. cta-ultra3, ISDC3700, ...  

   note 1: keywords ENERGYBINS and OUTPUTFILE are ignored in the runparameter file

   note 2: energy and wobble offset bins are hardwired in this scripts

   note 3: adjust h_cpu depending on your MVA method

   note 4: default TMVA parameter file is 
   \$CTA\_EVNDISP\_ANA\_DIR/ParameterFiles/TMVA.BDT.runparameter
\end{lstlisting}

Note that for off-axis training (parameter {\it cone10}), one MVA is produced per energy and off-axis bin.

\paragraph{Input}
Products from lookup analysis for gamma rays, protons and electrons. \\
TMVA run parameter from e.g. \\
{\it \$OBS\_EVNDISP\_ANA\_DIR/ParameterFiles/TMVA.BDT.runparameter} \\
run parameter file for scripts (see section \ref{SEC:scriptsRunParameter})

\paragraph{Output}

TMVA output files (ROOT and XML format) and log files are all written to \\
{\it \$CTA\_DATA\_DIR/analysis/SIMDATASET/TMVA/subDir/}.

Use the TMVA macros delivered with ROOT to check the gamma/hadron separation and training results (can be found in {\it \$ROOTSYS/tmva/test/TMVAGui.C}.


%###########################################################################
\subsection{Step 6: Effective area calculation}

Effective areas after optimal gamma-hadrons cuts are calculated in the following in three consecutive steps.
Cuts on the MVA variable are optimized 'on the fly', 
depending on the energy spectrum of the source, the source strength and the observation time.
First the typical angular resolution for the given array is calculated, then the signal and background 
rates after quality cuts, and finally, the optimal MVA cut value.

{\bf The $\Theta^2$-cut (cut on angular direction) is always the 80\% containment radius of the reconstructed with respect to the true direction. This might not the optimal choice, but it is probably reasonable for most point-source analysis.}

Note that the effective area calculation includes the determination of all instrument response functions (i.e. angular and energy resolution, migration matrix, etc).

\begin{lstlisting}
CTA.EFFAREA.sub_analyse.sh <subarray> <recid> <particle> <cutfile template> \
 <analysis parameter file> <outputsubdirectory> <data set> [filling mode]
================================================================================

make effective areas for CTA

(note: shape cuts hardwired)

<subarray>
     subarray for analysis (e.g. array E)

<recid>
     reconstruction ID from array reconstruction

<particle>
     gamma_onSource / gamma_cone10 / electron / 
     electron_onSource / proton / proton_onSource / helium 

<cutfile template>
     template for gamma/hadron cut file (full path and file name)

<analysis parameter file>
     file with analysis parameter

<outputsubdirectory>
     directory with all result and log files (full path)

  <data set>         e.g. cta-ultra3, ISDC3700m, ...  

[filling mode]
     effective area filling mode (use 2 to calculate angular resolution only)
\end{lstlisting}

Use {\it  ./CTA.EFFAREA.subAllParticle\_analyse.sh} to submit the jobs for all particle types.

\subsubsection{Directory structure for effective area calculation}

Note that the division into one directory per subarray configuration is not used here anymore.

\begin{itemize}
\item Results from angular resolution calculation: \\
{\it \$CTA\_USER\_DATA\_DIR/analysis/SIMDATASET/EffectiveArea/AngularResolution/}
\item Effective areas after quality ctus:\\
{\it \$CTA\_USER\_DATA\_DIR/analysis/EffectiveArea/SIMDATASET/QuallityCuts}
\end{itemize}

\subsubsection{Determination of angular resolution (80\% containment)} 

The angular resolution is a weak function of the MVA cut parameter. 
Therefore a MVA cut value corresponding to a fixed signal efficiency (20\%) 
in the following as applied.

\begin{lstlisting}
$EVNDISPSYS/scripts/CTA/CTA.EFFAREA.subAllParticles_analyse.sh \
subArray.list $CTA_EVNDISP_ANA_DIR/ParameterFiles \
ANASUM.GammaHadron.TMVAFixedSignal.gamma.dat \
$CTA_USER_DATA_DIR/analysis/EffectiveArea/SIMDATASET/AngularResolution \
<DataSet> 2
\end{lstlisting}

\paragraph{Input}
Products from lookup analysis for gamma rays, protons and electrons. \\
TMVA XML and root files with classifier data. \\
Cut parameter file: \\
{\it \$OBS\_EVNDISP\_ANA\_DIR/ParameterFiles/ANASUM.GammaHadron.TMVAFixedSignal}\\
run parameter file for scripts (see section \ref{SEC:scriptsRunParameter})

\paragraph{Output}

One ROOT file per subarray in the directory  \\
{\it \$CTA\_USER\_DATA\_DIR/analysis/EffectiveArea/SIMDATASET/AngularResolution} \\
To plot the angular resolution vs energy, follow the instructions described here in Section \ref{SEC.PLOTEFFAREA}.

\subsubsection{Effective areas after quality cuts}

This step is necessary for the determination of number of events after quality cuts. 
The effective areas after some loose quality cuts are determined, then folded with given spectra (e.g. Crab Nebula spectrum and CR proton and electron spectrum) followed by the calculation of the number of events expected for a certain observation period.

These are typical loose cuts applied in this step:
\begin{itemize}
\item successful reconstruction
\item $\Theta^2$ cut as described earlier (80\% containment radius, energy dependent)
\item $-2<MSCW<2$ and $-2<MSCL<5$
\end{itemize}
The cuts can be changed in the cut file, the default file is\\
{\it \$OBS\_EVNDISP\_ANA\_DIR/ParameterFiles/ANASUM.GammaHadron.QC}\\

To calculate effective areas after quality cuts do:

\begin{lstlisting}
$EVNDISPSYS/scripts/CTA/CTA.EFFAREA.subAllParticles_analyse.sh \
subArray.list $OBS_EVNDISP_ANA_DIR/ParameterFiles \
ANASUM.GammaHadron.QC \
$CTA_USER_DATA_DIR/analysis/EffectiveArea/<DataSet>/QualityCuts \
<DataSet> 
\end{lstlisting}

\paragraph{Input} 
Products from lookup analysis for gamma rays, protons and electrons. \\
Cut parameter file: \\
{\it \$OBS\_EVNDISP\_ANA\_DIR/ParameterFiles/ANASUM.GammaHadron.QC}\\
run parameter file for scripts (see section \ref{SEC:scriptsRunParameter})

\paragraph{Output}

One ROOT file per subarray in the directory  \\
{\it \$CTA\_USER\_DATA\_DIR/analysis/EffectiveArea/SIMDATASET/QualityCuts} \\
To plot the effective areas vs energy, follow the instructions described here in Section \ref{SEC.PLOTEFFAREA}.

\subsubsection{Calculation of number of signal and background events}

To calculate the number of signal and background events:

\begin{lstlisting}
% cd $CTA_USER_DATA_DIR/analysis/EffectiveArea/<DataSet>/QualityCuts
% root 
root [0] .L $EVNDISPSYS/lib/libVAnaSum.so
root [1] .L $EVNDISPSYS/macros/sensitivity.C
root [2] writeAllParticleNumberFiles( "subArray.list", 0 );
// or for several (e.g. 8) offsets:
root [2] writeAllParticleNumberFiles( "subArray.list", 8 );
\end{lstlisting}

\paragraph{Input}
Effective area files after quality cuts (one ROOT file per sub array). \\
Energy spectra for the Crab Nebula and proton and electron cosmic rays spectra are read from \\
{\it \$OBS\_EVNDISP\_ANA\_DIR/AstroData/TeV\_data/EnergySpectrum\_literatureValues\_CrabNebula.dat} \\
{\it \$OBS\_EVNDISP\_ANA\_DIR/AstroData/TeV\_data/EnergySpectrum\_literatureValues\_CR.dat} \\
run parameter file for scripts (see section \ref{SEC:scriptsRunParameter})

\paragraph{Output}

One ROOT file per subarray with two graphs: number of signal/number of background events per energy bin vs energy.

\subsubsection{Effective areas after gamma/hadron separation cuts}

Calculate effective areas while applying the TMVA-based gamma-hadron cuts
trained before.

Example:

\begin{lstlisting}
$EVNDISPSYS/scripts/CTA/CTA.EFFAREA.subAllParticles_analyse.sh \
subArray.list $CTA_EVNDISP_ANA_DIR/ParameterFiles \
ANASUM.GammaHadron.TMVA \
$CTA_USER_DATA_DIR/analysis/EffectiveArea/<DataSet>/TMVA/BDT.20120321 \
<DataSet> 
\end{lstlisting}

\paragraph{Input}
Products from lookup analysis for gamma rays, protons and electrons. \\
TMVA XML and root files with classifier data. \\
Cut parameter file: \\
{\it \$OBS\_EVNDISP\_ANA\_DIR/ParameterFiles/ANASUM.GammaHadron.TMVA}\\
run parameter file for scripts (see section \ref{SEC:scriptsRunParameter})

\paragraph{Output}
One effective area ROOT file per subarray with effective areas, angular and energy resolution, migration matrixes, etc.

%###########################################################################
\subsection{Step 7: Sensitivity curves \& WP-Phys files}

Calculates sensitivity curves and writes these together with instrument response histograms into ROOT files following the WP Phys format.

\begin{lstlisting}
$EVNDISPSYS/scripts/CTA/CTA.WPPhysWriter.sh <sub array list> \
<directory with effective areas> <observation time [h]> \
<output file name> <offset=0/1>
\end{lstlisting}

Some sensitivity related plotting functions can be found in the
root macro \$EVNDISPSYS/macros/sensitivity.C

\paragraph{Input}
Effective area file after gamma/hadron separation cuts

\paragraph{Output}
One root file per subarray with sensitivities and instrument response function histograms.

% ############ Input data format ######################
\chapter{Input data format}

\section{VBF - VERITAS Bank Format}

\section{DST - data summary tree}
\label{SECTION.DST}

The DST format is a simple ROOT tree containing standard C++ variables only (no class data).

\subsection{Limitations}
\label{SUBSECTION.DST.LIMITATIONS}

The implementation requires the hardwiring of the maximum number of telescopes, channels, etc. 
These values can be found in {\texttt inc/VGlobalRunParameter}, for example:

\begin{lstlisting}
// HARDWIRED MAXIMUM NUMBER OF TELESCOPES AND CHANNELS, etc.
// maximum number of telescopes
#define VDST_MAXTELESCOPES  100
// maximum number of channels per telescopes
#define VDST_MAXCHANNELS   12000
// maximum number of summation windows
// (=maximum number of samples per FADC trace)   
#define VDST_MAXSUMWINDOW   64
// maximum number of time slices for pedestal calculation
#define VDST_PEDTIMESLICES 5000   
// maximum number of arrayreconstruction method 
#define VDST_MAXRECMETHODS  100
// maximum number of timing levels
#define VDST_MAXTIMINGLEVELS 10    
\end{lstlisting}

\noindent {\bf NOTE: } These numbers determine the memory requirements of {\it evndisp} and {\it CTA.convert\_hessio\_to\_VDST}.

\noindent {\bf NOTE: } {\it evndisp} must be compiled with the same settings as the writing program.

% ############ detector geometry ######################

\chapter{Detector Setup}


\section{Telescope types}

Different telescope types (e.g. mid-size and small-size telescopes, telescopes with different FOV, etc) are assigned a telescope type number in the code, this number is as well written to the data trees. The telescope type  contains the mirror shape (DC, Parabolic, SC), the mirror area (m$^2$), the field of view ([deg]) and the pixel size ([deg]). For VERITAS, the telescope type correspond simply to the different telescope numbers (and are therefore 0,1,2,3).

For clarification, this is the corresponding code bit from {\texttt src/CTA.convert\_hessio\_to\_VDST.cpp}:

\begin{lstlisting}
fTelescope_type  = TMath::Nint(pix_size*100.);
fTelescope_type += TMath::Nint(fFOV*10.)*100;
fTelescope_type += TMath::Nint(fMirrorArea)*100*10*100;
// all large telescopes are parabolic, all others are Davies-Cotton (hardwired)
if( fMirrorArea > fParabolic_mirrorArea )      fTelescope_type += 100000000;
// Schwarzschild-Couder: check number of mirrors
else if( fNMirrors == fSC_number_of_mirrors )  fTelescope_type += 200000000;
\end{lstlisting}

\noindent {\bf Note:} There is currently no way to determine the mirror/telescope shape (parabolic, Davies-Cotton, etc) from the hessio file. This is why the mirror area and the number of mirrors is used. 
The parabolic shape is assigned to all telescopes with a mirror area  $>$ 400 m$^2$.
Schwarzschild-Couder Design are all telescopes with 2 mirrors only.

%############# VERITAS data analysis tools #################

\chapter{Tools for VERITAS analysis}

\section{Exposure maps and run lists for any point in the sky}

This small tools allows you to 

\begin{itemize}
\item plot exposure maps in Galactic coordinates for VERITAS (with and without radial acceptance)
\item get a list of data runs for a given direction in the sky
\item get a list of data runs for objects from a catalogue
\item print a LaTex-style table for the selected list of objects with exposure, zenith angles, observing angle, etc.
\end{itemize}

Usage example:

\begin{lstlisting}
 // load shared library
.L lib/libVAnaSum.so
VExposure f;
// read VERITAS db for a given period
// (only necessary if no root file with
// exposures is available)
f.setTimeRange("20070901", "20071231" );
f.readFromDB();
// the results can be saved to a root file (fast access)
f.writeRootFile( "myrootfile.root" );
// and later retrieved with
f.readRootFile("myrootfile.root" );
// for radial acceptance correction an acceptance curve is needed
f.readAcceptanceCurveFromFile("myacceptance.root" );
// fill maps: (this may take a while)
f.fillExposureMap();
// plot the maps for a given l, b range
f.plot(-10., 10., 60, 100., "" );
// plot the maps with some objects from a catalogue:
// (there are some examples of catalogues in the directory
// with the auxiliary files:
// AstroData/Catalogues/: tevcat.dat, ... 
f.plot(-10., 10., 60, 100., "mycatalogue.dat" );
// print list of runs of data taken e.g. around the Galactic centre (5 deg circle)
f.printListOfRuns( 0., 0., 5. ); 
// print list of runs around the objects in the given catalogue
f.printListOfRuns(  "mycatalogue.dat" );
\end{lstlisting}

\section{Radiosonde Atmospheric Data}

Download radiosonde balloon data from Tucson airport using a wget script and create a root file of the data.

Use the download script, for example get data for the entire year of 2010: 

\begin{lstlisting}
cd $EVNDISPSYS/scripts/VTS/
./VTS.downloadSoundingDatafromUWYO.sh 2010
\end{lstlisting}

Combine the monthly data into one file and create a list of files (in this case just the total file): 

\begin{lstlisting}
cat sounding_2010* > all_2010.dat
ls all_2010.dat > list_2010.dat
\end{lstlisting}

Use a root file of radiosonde data and plot results 

\begin{lstlisting}
root -l
.L ../../../../sharedLib/libVAnaSum.so 
VAtmosphereSoundings a;
a.readSoundingsFromTextFile("list_2010.dat");
a.writeRootFile("all_2010.root");
.q
\end{lstlisting}

Use the VAtmosphereSoundings class to plot the data: 

\begin{lstlisting}
root -l
gROOT->SetStyle("Plain");
gStyle->SetCanvasBorderMode(0);
gStyle->SetPadBorderMode(0);
gStyle->SetPalette(1); 
.L ../../../../sharedLib/libVAnaSum.so 
VAtmosphereSoundings a( "all_2010.root"); 
a.plotAverages(2010,1,2010,12);
a.plot2DProfiles(2010,1,2010,12);
.q
\end{lstlisting}

\begin{itemize}
\item please read through \$EVNDISPSYS/src/VAtmosphereSoundings.cpp for full analysis details 
\item for a detailed overview of the results please see: Atmosphere 
\item radiosonde data is from http://weather.uwyo.edu/upperair/sounding.html 
\end{itemize}

There is a root macro in {\it \$EVNDISPSYS/macros/VTS/atmosphericSounding.C} to plot monthly/yearly average and compare 
with atmospheric profiles from simulations.

% ############ light curve analysis ######################

\chapter{Light curve analysis}

Several methods for light curve analysis are currently under development. We describe in the following the existing tools, for further details please check the different sections in the code.

Light curve data can be read from evndisplay result files ({\it anasum} files) or from ascii files.

\section{Code organization}

\begin{itemize}
\item {\it VLightCurveData:} data class (contains a single point of the light curve)
\item {\it VLightCurveUtilities:} basic functions to read ascii files, print light curves, and get light curve properties (e.g. mean, variance)
\item {\it VLightCurve:} light curve reader and plotter for ascii and evndisplay result files ({\it anasum} files)
\item {\it  VLombScargle:} discrete Fourier transform for unevenly spaced data  after Lomb and Scargle
\item {\it VZDCF:} plotting class for Z-transformed discrete correlation functions
\end{itemize}

\section{Light curve plotting}

Example for plotting light curves using an evndisplay result files ({\it anasum} file:

\begin{lstlisting}
.L $EVNDISPSYS/lib/libVAnaSum.so 
VLightCurve *iLightCurve = new VLightCurve();
iLightCurve->setName( "VHE light curve" );
// iDayInterval = 15. e.g. 15 day binning
// iMJDmin and iMJDmax to restrict light curve range in MJD
iLightCurve->initializeTeVLightCurve( "myanasumfile.root", 
                       iDayInterval, iMJDmin, iMJDmax );
// plot points above 2 sigma or at least 3 on events as flux points, 
// all others as upper flux limits
iLightCurve->setSignificanceParameters( 2., 3. );
// for binaries: phase folding
iLightCurve->setPhaseFoldingValues( PhaseMJD0, PhaseOrbitdays );
// fill light curve with fluxes above 500 GeV
iLightCurve->fill( 0.5 );
// plot light curve
iLightCurve->plotLightCurve();
\end{lstlisting}

Text file example (observation date (MJD), length of observation (in days), flux and flux error):

\begin{lstlisting}
 54857.173780       0.074748        1.27275        0.17745  
 54858.178508       0.074864        1.27704        0.17042  
 54859.139628       0.101489        1.30742        0.18451  
 54860.154963       0.105847        1.31362        0.17415  
 54867.274058       0.262490        1.09824        0.15957  
\end{lstlisting}

Note the first two columns can be as well: begin and end of observations in MJD.

Example for plotting light curves using a simple text file:

\begin{lstlisting}
.L $EVNDISPSYS/lib/libVAnaSum.so 
VLightCurve b;
// plot 95\% upper flux limits for points with 
// significances $<2 \sigma$:
b.setSignificanceParameters( 2., -9999., 0.95 );
// set spectral parameters assumed in the flux calculation:
b.setSpectralParameters( 0., 1., -2.5 );
b.initializeXRTLightCurve("mylightcurve.txt");
b.setPlottingStyle( 2, 1, 1., 20, 1. );
b.setLightCurveAxis( 0., 2., "counting rate" );
b.plotLightCurve();
\end{lstlisting}

Additionally to the described functions, there are several functions to fill gaps in light curves. 
This is work in progress and should be used only after carefully reading of the code.

{\it VLightCurve} and {\it VLightCurveUtillities} provide several methods to print details of the calculations to the screen and fill Latex and Wiki tables.

\section{Lomb Scargle analysis}

The discrete Fourier transform for unevenly spaced data  after Lomb and Scargle is implemented in the {\it VLombScargle} class \cite{Scargle-1982}. 
The basic light curve reader classes can be used to read in a light curve from a text or eventdisplay result file.

There are two different implementations for the calculation of the significances of the peaks:
\begin{enumerate}
\item the calculation provided by Lomb \& Scargle taking into account the number of independent frequencies scanned and Poissonian errors
\item a toy MC based method: the light curve is randomly shuffled $N$ times with the flux points changed randomly according to their errors. For each of the resulting light curve the periodigram is calculated. The probability/significance is derived from the distribution of powers at each frequency bin (note: a large number of toy light curves have to be produced for larger values of significance).
\end{enumerate}

Example:

\begin{lstlisting}
.L $EVNDISPSYS/lib/libVAnaSum.so 
 VLombScargle g;
g.readASCIIFile("mylightcurve.txt");
// scan 1000 frequencies between 50. and 1000. days
g.setFrequencyRange(1000., 1./1000., 1./50. );
g.plotPeriodigram();
// plot a line at the give frequency
g.plotFrequencyLine(1./315. );
// plot probability levels using the Lomb \& Scargle calculation
g.plotProbabilityLevels();
// plot probability levels using a toy MC with 500 MC light curve realisations
g.plotProbabilityLevelsFromToyMC( 500);
\end{lstlisting}

\section{ZDCF Autocorrelation analysis}

No autocorrelation analysis is implemented yet. We used until now the Z-transformed discrete correlation functions and the code provided by the authors of ZDCF\footnote{http://www.weizmann.ac.il/weizsites/tal/research/software/}.

There is a small class in eventdisplay provided to plot the ZDCF results, see the following example:

\begin{lstlisting}
.L $EVNDISPSYS/lib/libVAnaSum.so 
VZDCF a;
a.readZDCF( "XRT20120216.dcf");
a.setMLinterval( 315., 315.+6., 315.-3.86 );  // error provided by plike
a.plotZDCFoverError( 0, 45., 405., 12. );
a.plotZDCF( 0, 45., 405. );
\end{lstlisting}



% ############ Appendix ######################

%\begin{appendix}
 %coordinate systems
%\end{appendix}


%\bibliographystyle{plain_gm}
%\bibliographystyle{hep}
%\cleardoublepage
%\addcontentsline{toc}{chapter}{\bibname}
%\bibliography{BibAstro,BibStat,BibPhys,BibPart,BibEtc,BibKASCADE}

% #############################################

\begin{thebibliography}{}
\bibitem[Scargle(1982)]{Scargle-1982} Scargle, J. 1982, ApJ 263, 835
\end{thebibliography}

\end{document}
